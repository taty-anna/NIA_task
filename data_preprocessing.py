"""This script is used for data cleaning and preparing the data for analysis."""import osimport pandas as pdimport numpy as npimport loggingfrom datetime import datetimeimport warningsimport redef setup_logging():    """Set up logging configuration. Generates a log file inside the 'logs' directory."""    log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')    os.makedirs(log_dir, exist_ok=True)    log_file = os.path.join(log_dir, f'data_cleaning_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')    # Configure the root logger instead of creating a new logger    logger = logging.getLogger()  # Get the root logger    logger.setLevel(logging.INFO)    # Create handlers    file_handler = logging.FileHandler(log_file)    stream_handler = logging.StreamHandler()    formatter = logging.Formatter('%(asctime)s : %(module)s - %(levelname)s - %(message)s',                                  datefmt='%Y-%m-%d %H:%M:%S')    file_handler.setFormatter(formatter)    stream_handler.setFormatter(formatter)    # Add handlers to the logger    logger.addHandler(file_handler)    logger.addHandler(stream_handler)def load_and_display_data(file_name):    """    Read a CSV file and return the data as a pandas DataFrame.    Display basic debug logs.    """    try:        input_df = pd.read_csv(file_name, usecols=[1, 2], names=['time', 'prices'], engine='python', skiprows=1,                               skip_blank_lines=True)        logging.info("Data loaded successfully.")        # delimiter is not consistent in all the rows/columns, need to remove unwanted spaces to avoid data loss:        input_df['time'] = input_df['time'].str.replace(" ", "")  # remove unwanted spaces        input_df['time'] = input_df['time'].apply(lambda x: x[:10] + ' ' + x[10:] if x is not None else None)    except FileNotFoundError:        logging.error(f"The file {file_name} does not exist.")        raise Exception(f"The file {file_name} does not exist.")    except pd.errors.ParserError:        logging.error("Error occurred while loading data: File content might be malformed or not in expected format")        raise Exception("Error occurred while loading data: File content might be malformed or not in expected format")    except Exception as e:        logging.error(f"An unexpected error occurred while loading data: {str(e)}")        raise Exception(f"An unexpected error occurred while loading data: {str(e)}")    return input_dfdef detect_and_remove_outliers_iqr(df: pd.DataFrame, col):    # Compute boundaries for outliers    q1 = df[col].quantile(0.25)    q3 = df[col].quantile(0.75)    iqr = q3 - q1    # Identify the outliers    outliers_bool = ((df[col] < (q1 - 1.5 * iqr)) | (df[col] > (q3 + 1.5 * iqr)))    # Log the outlier rows    outliers = df[outliers_bool]    logging.info(f"Identified outliers in {col}:\n{outliers}")    # Count the number of outliers    outlier_count = outliers_bool.sum()    print(f"The 'price' column contains {outlier_count} outliers.")    # Replace outliers with NaN    df_outliers_replaced = df.copy()    df_outliers_replaced.loc[outliers_bool, col] = np.nan    logging.info(f'Outliers replaced with NaN in {col} using the IQR method.')    return df_outliers_replaceddef remove_duplicates(df: pd.DataFrame, col):    # Check for duplicates in the specified columns, remove and keep first.    duplicate_rows = df[df.duplicated(subset=col)]    if len(duplicate_rows) > 0:        logging.info(f"Found {len(duplicate_rows)} duplicate rows.")        logging.info(f"The duplicated rows are:\n{duplicate_rows}")    df_no_duplicates = df.drop_duplicates(subset=col, keep='first')    return df_no_duplicatesdef check_frequency(df: pd.DataFrame, col):    freq = '30T'  # Set desired frequency (30 minutes)    df[col] = pd.to_datetime(df[col], format='%d/%m/%Y %H:%M')    df.set_index(col, inplace=True)    # Resample with frequency    df_resampled = df.asfreq(freq)    # Interpolate missing values with linear method    df_resampled_interpolated = df_resampled.interpolate(method='linear')    logging.info("Data has been resampled and interpolated to match the desired frequency.")    # Reset the index and rename the new time column    df_resampled_interpolated.reset_index(inplace=True)    df_resampled_interpolated.rename(columns={df_resampled_interpolated.columns[0]: 'time'}, inplace=True)    return df_resampled_interpolatedsetup_logging()def preprocess_data(file_name: str) -> pd.DataFrame:    """    Main function to preprocess data    Parameters:        file_name(str): The path to the CSV file    Returns:        pd.DataFrame: Preprocessed price data as a DataFrame    """    df = load_and_display_data(file_name)    logging.info(f"Data consists of {len(df)} observations")    df['prices'] = pd.to_numeric(df['prices'], errors='coerce')  # ensure 'prices' column is a number    df['time'] = pd.to_datetime(df['time']).dt.strftime('%d/%m/%Y %H:%M')   # convert 'time' column    # Continue the preprocess operations on df    preprocessed_data = (df                         .pipe(remove_duplicates, ['time'])  # remove duplicates                         .pipe(detect_and_remove_outliers_iqr, 'prices')  # remove outliers                         .pipe(check_frequency, 'time')  # handle frequency                         )    preprocessed_data['prices'] = preprocessed_data['prices'].round(2)  # round price to 2 decimal places    # Final checks:    # expect no NaN values, no duplicates, correct data types, no outliers, and correctly rounded price column    assert preprocessed_data.notna().all().all(), "Unexpected NaN values in the data"    assert not preprocessed_data['time'].duplicated().any(), "Unexpected duplicate values in the data"    assert preprocessed_data.dtypes.to_dict() == {'time': 'datetime64[ns]', 'prices': 'float64'}, ("Data types are not "                                                                                                   "as expected")    assert (preprocessed_data['prices'] == preprocessed_data['prices'].round(2)).all(), ("Price values are not "                                                                                         "correctly rounded to 2 "                                                                                         "decimal places")    logging.info(f"Exit with {len(preprocessed_data)} observations")    final = preprocessed_data.to_csv("preprocessed_data.csv")    return preprocessed_data